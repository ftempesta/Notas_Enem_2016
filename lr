#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Jun  7 23:49:23 2020

@author: matheus
"""


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import xgboost
import numpy as np
import pickle 
from statsmodels.regression.linear_model import OLSResults
from sklearn.linear_model import LassoCV

from codenation_library_matheus import *

# ============================================================
# Explorar datos
# ============================================================

# Load_data
dataset = pd.read_csv('testfiles/train.csv', index_col=0)
dataset_test = pd.read_csv('testfiles/test.csv')

columns_test = dataset_test.columns.to_list()
columns_test.append('NU_NOTA_MT')

dataset = dataset[columns_test]


# Describe
dims = dataset.shape
describe = dataset.describe()

# ============================================================
# Eliminete NaN's in NU_NOTA_MT and irrelevats columns
# ============================================================

# Drop rows with nan values in the NU_NOTA_MT column
nulos = dataset.isnull()['NU_NOTA_MT'].value_counts()
dataset = dataset.dropna(subset=['NU_NOTA_MT']).reset_index(drop=True)


# Delete irrelevant columns 
columns_dataset = dataset.columns.to_list()

# columns_to_delete = ['NU_ANO', 'CO_MUNICIPIO_RESIDENCIA',
#                      'SG_UF_RESIDENCIA', 'CO_MUNICIPIO_NASCIMENTO',
#                      'SG_UF_NASCIMENTO', 'CO_ESCOLA', 'CO_MUNICIPIO_ESC', 
#                      'SG_UF_ESC', 'SG_UF_ENTIDADE_CERTIFICACAO',
#                      'CO_MUNICIPIO_PROVA', 'SG_UF_PROVA', 'CO_PROVA_CN',
#                      'CO_PROVA_CH', 'CO_PROVA_LC', 'CO_PROVA_MT',
#                      'TX_RESPOSTAS_CN', 'TX_RESPOSTAS_CH', 'TX_RESPOSTAS_LC',
#                      'TX_RESPOSTAS_MT', 'TX_GABARITO_CN', 'TX_GABARITO_CH',
#                      'TX_GABARITO_LC', 'TX_GABARITO_MT', 'TP_STATUS_REDACAO',
#                      'NU_NOTA_COMP1', 'NU_NOTA_COMP2', 'NU_NOTA_COMP3',
#                      'NU_NOTA_COMP4', 'NU_NOTA_COMP5',
#                      'NO_MUNICIPIO_NASCIMENTO']

# dataset = dataset.drop(columns=columns_to_delete)

# ============================================================
# Eliminete columns with less than 50 % of the values
# ============================================================

# Count nan values by columns
null_columns =\
    pd.DataFrame(dataset.isnull().sum(axis=0)).reset_index(drop=False)
null_columns.columns = ['columns', 'count_nan_values']

# Calculated percentage of NON null values 
sum_values = dataset.shape[0]
null_columns['percentage_non_nan'] =\
    (1 - (null_columns['count_nan_values'] / sum_values)) * 100

# Select only the columns with less than 50 % of nan's values
null_columns_filtered = null_columns[null_columns['percentage_non_nan'] > 50]
columns_correct = null_columns_filtered['columns'].to_list()

# Filter dataset
dataset = dataset[columns_correct]

# Filtered dataset with the columns selected before 
dataset = dataset_filtered.dropna()

# Target: inscription number and score math testv
target = dataset[['NU_INSCRICAO','NU_NOTA_MT']]
dataset = dataset.drop(columns=['NU_INSCRICAO','NU_NOTA_MT'])


# ============================================================
# Transform categorical to numeric
# ============================================================

categorical_columns =\
     dataset.select_dtypes(include=['object']).columns.to_list()
dataset = labeL_encoder(dataset, categorical_columns)

# Apply label encoder
dataset_filtered = labeL_encoder(dataset_filtered, categorical_columns)
total_columns = dataset.columns.to_list()

# dataset = dataset[['NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC']]
# ============================================================
# Data preparation
# ============================================================

# correlation_feature_important =\
#     dataset.corr().unstack().\
#         sort_values().drop_duplicates().reset_index(drop=False)

# correlation_feature_important.columns = ["var1", "var2", "correlation"]

# correlation_feature_important =\
#     correlation_feature_important.sort_values(by=["correlation"],
#                                                       ascending=False).\
#     iloc[1:].reset_index(drop=True)

# # High correlation columns
# high_correlated_columns =\
#     correlation_feature_important[(correlation_feature_important['correlation'] > 0.25) |
#                         (correlation_feature_important['correlation'] < -0.25)]

# good_correlation =\
#     correlation_feature_important[(correlation_feature_important['correlation'] < 0.005) &
#                         (correlation_feature_important['correlation'] > -0.005)]



# var1 = pd.DataFrame(good_correlation["var1"]).reset_index(drop=True).values.tolist()
# var2 = pd.DataFrame(good_correlation["var2"]).reset_index(drop=True).values.tolist()

# l = []

# for i in range(len(var1)):
#     l.append(var1[i])
# # for i in range(len(var2)):
# #     l.append(var2[i])
    
# variables = pd.DataFrame(l)[0].unique().tolist()

# dataset = dataset[variables]

dataset = dataset.reset_index(drop=True)
dataset_filtered = dataset_filtered.reset_index(drop=True)



indices_list = dataset.index.to_list()

train_indices, test_indices = train_test_split(indices_list, test_size=0.2)

X = dataset
columns_test = X.columns.to_list()


# Aplicar PCA para encontrar componentes principales
# pca = PCA(n_components=10)
# principalComponents = pca.fit_transform(X)
# principalComponents = pd.DataFrame(principalComponents)

# X = principalComponents
# Columns 



columns_dataset = X.columns
y = target["NU_NOTA_MT"]


# Normalize features and split
sc_X = MinMaxScaler(feature_range = (0,1))
X = sc_X.fit_transform(X)

sc_y = MinMaxScaler(feature_range = (0,1))
y = y.to_numpy()
y = y.reshape(-1,1)
y = sc_y.fit_transform(y)



# Min and max to rescale the data
min_y = sc_y.data_min_
min_y = min_y[0]
max_y = sc_y.data_max_
max_y = max_y[0]

min_x = sc_X.data_min_
min_x = min_x[0]
max_x = sc_X.data_max_
max_x = max_x[0]



dataset_final_test = dataset_test[columns_test].reset_index(drop=True)
inscription_number_test = pd.DataFrame(dataset_test['NU_INSCRICAO'])


pkl_file = open('label_encoding.pkl', 'rb')
label_encoder = pickle.load(pkl_file) 
pkl_file.close()


for column in dataset_final_test.columns:
    dataset_final_test[column] =\
        label_encoder.fit_transform(dataset_final_test[column])

dataset_final_test = dataset_final_test.reset_index(drop=True)
dataset_final_test = dataset_final_test.fillna(dataset_final_test.mean())

X_final_test = dataset_final_test
# X_final_test = pca.fit_transform(X_final_test)
X_final_test = pd.DataFrame(X_final_test)

# Normalize features 
X_final_test = sc_X.fit_transform(X_final_test)



reg = LassoCV()
reg.fit(X, y)

predictions = reg.predict(X_final_test)
print("Best alpha using built-in LassoCV: %f" % reg.alpha_)
print("Best score using built-in LassoCV: %f" %reg.score(X,y))
coef = pd.Series(reg.coef_, index = X.columns)

print("Lasso picked " + str(sum(coef != 0)) +
      " variables and eliminated the other " +
      str(sum(coef == 0)) + " variables")

imp_coef = coef.sort_values()
import matplotlib
matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)
imp_coef.plot(kind = "barh")
plt.title("Feature importance using Lasso Model")






model = LinearRegression()

model.fit(X, y)
predictions = model.predict(X_final_test)
predictions = pd.DataFrame(predictions).to_numpy()

predictions = predictions.reshape(-1)
predictions = predictions*(max_y-min_y)+min_y
predictions = pd.DataFrame(predictions)










resultados_finales =\
    pd.concat([inscription_number_test, predictions], axis=1)


resultados_finales.columns = ['NU_INSCRICAO', 'NU_NOTA_MT']

resultados_finales.to_csv('answer.csv', index=False)
